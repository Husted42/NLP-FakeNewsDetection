{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\alx26\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\alx26\\anaconda3\\lib\\site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\alx26\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\alx26\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "!pip install gensim\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV file, leaving only the collumns we might be interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates:  10211\n",
      "Fake / reliable\n",
      "RangeIndex(start=0, stop=52645, step=1)\n",
      "RangeIndex(start=0, stop=27660, step=1)\n",
      "\n",
      "loaded index\n",
      "RangeIndex(start=0, stop=100000, step=1)\n",
      "\n",
      "filtered\n",
      "       Unnamed: 0      id        type  \\\n",
      "0              10      17  conspiracy   \n",
      "1              11      18  conspiracy   \n",
      "2              12      19  conspiracy   \n",
      "3              13      20  conspiracy   \n",
      "4              56      68  conspiracy   \n",
      "...           ...     ...         ...   \n",
      "80298        7790   96593    reliable   \n",
      "80300        1189  101029    reliable   \n",
      "80301        4169  105092    reliable   \n",
      "80302        4440  105498    reliable   \n",
      "80304        9582  112137    reliable   \n",
      "\n",
      "                                                 content  \\\n",
      "0      \\n\\n\\n\\n\\n\\n\\n\\nRev Dr. Childress is available...   \n",
      "1      \\n\\nSpeaking Engagement Request\\n\\n\\n\\nContact...   \n",
      "2      \"…I have set before you life and death, blessi...   \n",
      "3      Why We Oppose Planned Parent Hood ( The follow...   \n",
      "4           If You Love The Children\\n\\nSaySo March 2014   \n",
      "...                                                  ...   \n",
      "80298  Amid the debate over whether \"fundamentalist C...   \n",
      "80300  It's the football off-season and Denver Bronco...   \n",
      "80301  It's illegal for Christians to pray in public ...   \n",
      "80302  It's illegal for Christians to pray in public ...   \n",
      "80304  The views expressed by the author do not neces...   \n",
      "\n",
      "                      inserted_at  \\\n",
      "0      2018-02-02 01:19:41.756632   \n",
      "1      2018-02-02 01:19:41.756632   \n",
      "2      2018-02-02 01:19:41.756632   \n",
      "3      2018-02-02 01:19:41.756632   \n",
      "4      2018-02-02 01:19:41.756632   \n",
      "...                           ...   \n",
      "80298  2018-02-02 01:19:41.756632   \n",
      "80300  2018-02-02 01:19:41.756632   \n",
      "80301  2018-02-02 01:19:41.756632   \n",
      "80302  2018-02-02 01:19:41.756632   \n",
      "80304  2018-02-02 01:19:41.756632   \n",
      "\n",
      "                                        meta_description  source  \n",
      "0      Find out when and where you can get involved w...     NaN  \n",
      "1                                                    NaN     NaN  \n",
      "2      The Negro Project is a devastating eugenic sta...     NaN  \n",
      "3      A rationale for opposing the work of Planned P...     NaN  \n",
      "4                                                    NaN     NaN  \n",
      "...                                                  ...     ...  \n",
      "80298  Amid the debate over whether \"fundamentalist C...     NaN  \n",
      "80300  It's the football off-season and Denver Bronco...     NaN  \n",
      "80301  It's illegal for Christians to pray in public ...     NaN  \n",
      "80302                                                NaN     NaN  \n",
      "80304  This presidential election confronts Christian...     NaN  \n",
      "\n",
      "[70094 rows x 7 columns]\n",
      "\n",
      "Creates a csv file called: readyData.csv\n"
     ]
    }
   ],
   "source": [
    "def getParts():\n",
    "    dropList = ['domain', 'url', 'scraped_at', 'updated_at', 'title', 'authors', 'keywords', 'meta_keywords', 'tags', 'summary' ] # 'source'\n",
    "\n",
    "    df = pd.read_csv('csvFile.csv', nrows=100000)\n",
    "    \n",
    "    # Filter fake\n",
    "    df_fake = df.loc[df['type'] == 'fake']\n",
    "    df_conspiracy = df.loc[df['type'] == 'conspiracy']\n",
    "\n",
    "\n",
    "    #filter reliable\n",
    "    df_reliable = df.loc[df['type'] == 'reliable']\n",
    "    df_political = df.loc[df['type'] == 'political']\n",
    "\n",
    "    #Concat\n",
    "    df_reliable = pd.concat([df_political, df_reliable], ignore_index=True)\n",
    "    df_fake = pd.concat([df_conspiracy, df_fake], ignore_index=True)\n",
    "    df_filtered = pd.concat([df_fake, df_reliable], ignore_index=True)\n",
    "\n",
    "    # Write DataFrame to CSV file\n",
    "    print(\"duplicates: \", df_filtered.duplicated(subset=['content']).sum())\n",
    "    df_filtered = df_filtered.drop(dropList, axis=1)\n",
    "    df_filtered.drop_duplicates(subset=['content'], inplace=True)\n",
    "    df_filtered.to_csv('readyData.csv', index=False)\n",
    "    \n",
    "    print(\"Fake / reliable\")\n",
    "    print(df_fake.index)\n",
    "    print(df_reliable.index)\n",
    "    print(\"\")\n",
    "    print(\"loaded index\")\n",
    "    print(df.index)\n",
    "    print(\"\")\n",
    "    print(\"filtered\")\n",
    "    print(df_filtered)\n",
    "    print(\"\")\n",
    "    print(\"Creates a csv file called: readyData.csv\")\n",
    "getParts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data overview 1 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types:\n",
      "Unnamed: 0            int64\n",
      "id                    int64\n",
      "type                 object\n",
      "content              object\n",
      "inserted_at          object\n",
      "meta_description     object\n",
      "source              float64\n",
      "dtype: object\n",
      "\n",
      "Count of Null: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0              0\n",
       "id                      0\n",
       "type                    0\n",
       "content                 0\n",
       "inserted_at             0\n",
       "meta_description    54822\n",
       "source              70094\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data01 = pd.read_csv('readyData.csv')\n",
    "def types(inp):\n",
    "    results = inp\n",
    "    results = results.dtypes\n",
    "    return results\n",
    "\n",
    "print(\"Types:\")\n",
    "print(types(data01))\n",
    "\n",
    "print(\"\")\n",
    "print ('Count of Null: ' )\n",
    "data01.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preproccesing #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m     inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mcleanedNews.csv\u001b[39m\u001b[39m'\u001b[39m, index \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m \u001b[39m##### -- Calls -- #####\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m run(createDataframe(fileRaw))\n\u001b[0;32m     54\u001b[0m \u001b[39mprint\u001b[39m(createDataframe(testSample)[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     55\u001b[0m \u001b[39mprint\u001b[39m(createDataframe(testSample)[\u001b[39m'\u001b[39m\u001b[39mmeta_description\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m, in \u001b[0;36mcreateDataframe\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreateDataframe\u001b[39m(\u001b[39minput\u001b[39m): \n\u001b[0;32m     17\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39minput\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     df \u001b[39m=\u001b[39m cleanContent(df, \u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m     df \u001b[39m=\u001b[39m cleanContent(df, \u001b[39m'\u001b[39m\u001b[39mmeta_description\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
      "Cell \u001b[1;32mIn[1], line 30\u001b[0m, in \u001b[0;36mcleanContent\u001b[1;34m(input, columnName)\u001b[0m\n\u001b[0;32m     28\u001b[0m regexList \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     29\u001b[0m pattern \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(regexList))\n\u001b[1;32m---> 30\u001b[0m \u001b[39minput\u001b[39m[columnName] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m[columnName]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: pattern\u001b[39m.\u001b[39;49msub(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, x))\n\u001b[0;32m     31\u001b[0m \u001b[39minput\u001b[39m[columnName] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[columnName]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mphone\u001b[39m\u001b[39m'\u001b[39m, regex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m \u001b[39minput\u001b[39m[columnName] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[columnName]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps?://\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mbhttp://\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m, regex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\alx26\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\alx26\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\alx26\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1144\u001b[0m             values,\n\u001b[0;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1147\u001b[0m         )\n\u001b[0;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\alx26\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[1], line 30\u001b[0m, in \u001b[0;36mcleanContent.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     28\u001b[0m regexList \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     29\u001b[0m pattern \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(regexList))\n\u001b[1;32m---> 30\u001b[0m \u001b[39minput\u001b[39m[columnName] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[columnName]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: pattern\u001b[39m.\u001b[39msub(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, x))\n\u001b[0;32m     31\u001b[0m \u001b[39minput\u001b[39m[columnName] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[columnName]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+-\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mphone\u001b[39m\u001b[39m'\u001b[39m, regex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m \u001b[39minput\u001b[39m[columnName] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[columnName]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps?://\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+|\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mbhttp://\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m, regex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### -- Imports -- #####\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Remember to run the line below the first time \n",
    "# nltk.download('punkt') \n",
    "\n",
    "fileRaw = 'readyData.csv'\n",
    "\n",
    "\n",
    "''' Creates dataframe (Run fuctions) '''\n",
    "def createDataframe(input): \n",
    "    df = pd.read_csv(input)\n",
    "    df = cleanContent(df, 'content')\n",
    "    df = cleanContent(df, 'meta_description')\n",
    "    return df\n",
    "\n",
    "''' Cleans and tokenizes text  '''\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "def cleanContent(input, columnName):\n",
    "    input[columnName] = input[columnName].str.lower()\n",
    "    regexList = ['\\.', ':', '&', ',', '\\?', ' us ', '!', ';', '\\$', '%', '\\(', '\\)', '\\[', '\\]']\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    regexList += [r'\\b{}\\b'.format(word) for word in stop_words]\n",
    "    pattern = re.compile('|'.join(regexList))\n",
    "    input[columnName] = input[columnName].apply(lambda x: pattern.sub('', x))\n",
    "    input[columnName] = input[columnName].str.replace(r'\\b\\d+-\\d+-\\d+-\\d+\\b', 'phone', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r'https?://\\S+|\\bhttp://\\S+', 'url', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r'\\b\\d+\\b', 'number', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r\"\\b\\w\\b\\s?\\b\", '', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r\"['`.*@-]\", '', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r'\\s+', ' ', regex=True)\n",
    "    for i in range(0, len(input[columnName])):\n",
    "        colElm = input.at[i, columnName]\n",
    "        if not colElm.isspace() and colElm != \"\":\n",
    "            colElm = nltk.word_tokenize(colElm)\n",
    "            stemmed_words = []\n",
    "            for word in colElm:\n",
    "                stemmed_words.append(sno.stem(word))\n",
    "            stemmed_words = ' '.join(stemmed_words)\n",
    "            input.at[i, columnName] = stemmed_words\n",
    "    return input\n",
    "\n",
    "'''Converts to csv File'''\n",
    "def run(inp):\n",
    "    inp = inp.to_csv('cleanedNews.csv', index = True)\n",
    "\n",
    "##### -- Calls -- #####\n",
    "run(createDataframe(fileRaw))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data overview 2 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words in clean and raw\n",
      "['number', '’', '“', '”', 'one', 'trump', 'state', 'peopl', 'would', 'time', 'year', 'like', 'blockchain', 'new', 'said', 'use', 'also', '–', 'report', 'make', 'get', 'go', 'next', 'even', 'two', '``', 'market', '01:19:41.756632,nan,', 'presid', 'govern', 'say', 'think', 'american', 'mani', '—', 'work', 'first', 'day', 'world', 'right', 'stock', 'nation', 'could', 'take', 'come', 'way', \"''\", 'need', 'may', 'know', 'search', 'see', 'bitcoin', 'call', 'want', 'sourc', 'exceed', 'fact', 'obama', 'includ', 'headlin', 'thing', 'countri', 'well', 'last', 'stori', 'news', 'much', 'support', 'back', 'hous', 'law', 'live', 'look', 'polit', 'democrat', 'good', 'unit', '‘', 'power', 'public', 'help', 'show', 'war', 'week', 'america', 'republican', 'follow', 'chang', 'made', 'life', 'group', 'part', 'end', 'person', 'inform', 'system', 'sinc', 'point', 'continu']\n",
      "['the', 'of', 'to', 'and', 'a', 'in', 'is', 'that', 'for', 'on', 'are', 'as', 'with', 'it', 'this', 'be', 'by', 'was', 'have', 'i', 'you', 'from', 'at', 'not', 'they', 'or', 'we', 'has', 'he', 'but', 'an', 'will', 'his', 'their', 'who', 'all', 'more', 'about', 'your', 'if', 'one', 'can', 'our', 'what', 'would', 'been', 'which', 'when', 'were', 'so', 'blockchain', 'out', 'there', 'people', 'new', 'had', 'no', 'my', 'up', 'than', 'its', '–', 'also', 'just', 'other', 'some', 'like', 'do', 'into', 'these', '&', 'over', 'two', '01:19:41.756632,,', 'how', 'us', 'only', 'any', 'after', 'even', 'most', 'many', 'because', 'said', 'those', '—', 'time', 'could', 'now', 'them', 'first', 'get', 'trump', 'her', 'may', 'it’s', 'being', 'think', 'bitcoin', 'stocks']\n",
      "\n",
      "Words that apear in both list of words:\n",
      "['one', 'trump', 'would', 'time', 'like', 'blockchain', 'new', 'said', 'also', '–', 'get', 'even', 'two', 'think', '—', 'first', 'could', 'may', 'bitcoin']\n"
     ]
    }
   ],
   "source": [
    "##### -- Imports -- #####\n",
    "import matplotlib.pyplot as plt \n",
    "import itertools\n",
    "\n",
    "##### --  Global variables -- #####\n",
    "fileCleaned = 'cleanedNews.csv'\n",
    "\n",
    "\n",
    "\n",
    "# dfClean = pd.read_csv(nameOfCleanedCSV)\n",
    "# dfRaw = pd.read_csv(nameOfRawCSV)\n",
    "\n",
    "##### --  Functions -- #####\n",
    "''' Creates dictionary of 100 most used times'''\n",
    "def wordDic(input):\n",
    "    file = open(input, 'r',  errors=\"surrogateescape\")\n",
    "    read = file.read().lower()\n",
    "    words = read.split()  \n",
    "    dictionary = {}\n",
    "    for i in words:\n",
    "        if i in dictionary:\n",
    "            dictionary[i] += 1  \n",
    "        else:\n",
    "            dictionary[i] = 1\n",
    "    sort = dict(sorted(dictionary.items(), key=lambda x: x[1], reverse=True))\n",
    "    words = dict(itertools.islice(sort.items(), 100))\n",
    "    return words\n",
    "\n",
    "def something(input):\n",
    "    resultList = list(wordDic(input).items())\n",
    "    lst = []\n",
    "    for elm in resultList:\n",
    "        lst.append(elm[0])\n",
    "    return lst\n",
    "\n",
    "\n",
    "##### --  Prints -- #####\n",
    "print(\"Most used words in clean and raw\")\n",
    "mostUsedWordsClean = (something(fileCleaned))\n",
    "mostUsedWordsRaw = (something(fileRaw))\n",
    "print(mostUsedWordsClean)\n",
    "print(mostUsedWordsRaw) \n",
    "\n",
    "print(\"\\nWords that apear in both list of words:\")\n",
    "doubleAppearances = []\n",
    "for word in mostUsedWordsClean:\n",
    "    if word in mostUsedWordsRaw:\n",
    "        doubleAppearances.append(word)\n",
    "print(doubleAppearances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code block solves the following tasks:\n",
    " - Cheks the effect of the cleaning, based on the words apearing in the most used list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word number apears: 668333 times\n",
      "the word phone apears: 7696 times\n",
      "the word url apears: 168 times\n"
     ]
    }
   ],
   "source": [
    "def allWordsToDic(input):\n",
    "    file = open(input, 'r',  errors=\"surrogateescape\")\n",
    "    read = file.read().lower()\n",
    "    words = read.split()  \n",
    "    dictionary = {}\n",
    "    for i in words:\n",
    "        if i in dictionary:\n",
    "            dictionary[i] += 1  \n",
    "        else:\n",
    "            dictionary[i] = 1\n",
    "    return dictionary\n",
    "\n",
    "print(\"the word number apears: \" + str(allWordsToDic(fileCleaned)['number']) + ' times')\n",
    "print(\"the word phone apears: \" + str(allWordsToDic(fileCleaned)['phone']) + ' times')\n",
    "print(\"the word url apears: \" + str(allWordsToDic(fileCleaned)['url']) + ' times')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ready data for baseline model #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "80% of DataFrame:\n",
      "(56075, 8)\n",
      "\n",
      "10% of DataFrame:\n",
      "(7010, 8)\n",
      "\n",
      "rest of the 10% of DataFrame:\n",
      "(7009, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "df = pd.read_csv(('cleanedNews.csv'))\n",
    " \n",
    "# Creating a dataframe with 80% of the data\n",
    "part_80 = df.sample(frac = 0.8)\n",
    "part_80.to_csv('split80_train.csv', index=False)\n",
    "\n",
    "# Creating a dataframe with the rest (20%)\n",
    "rest_part = df.drop(part_80.index) \n",
    "rest_part.to_csv('split20_temp.csv', index=False)\n",
    "\n",
    "# Creating a new dataframe to split the 20 % for test and validation\n",
    "df10 = pd.read_csv('split20_temp.csv')\n",
    "\n",
    "# Creating a dataframe with 50% of the data / 10% of the whole dataset \n",
    "part_50 = df10.sample(frac=0.5)\n",
    "part_50.to_csv('split10_test.csv', index=False)\n",
    "\n",
    "# Creating a dataframe with 50% of the data / 10% of the whole dataset \n",
    "part_50_2 = df10.drop(part_50.index)\n",
    "part_50_2.to_csv('split10_val.csv', index=False)\n",
    "\n",
    "print(\"\\n80% of DataFrame:\")\n",
    "print(part_80.shape)\n",
    "\n",
    "print(\"\\n10% of DataFrame:\")\n",
    "print(part_50.shape)\n",
    "\n",
    "print(\"\\nrest of the 10% of DataFrame:\")\n",
    "print(part_50_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from collections import Counter\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "train = pd.read_csv('split80_train.csv')\n",
    "test = pd.read_csv('split10_test.csv')\n",
    "val = pd.read_csv('split10_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstOfWordsInWv = []\n",
    "lstOfWordsNotInWv = []\n",
    "\n",
    "def sent_vec(sent):\n",
    "    vector_size = wv.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in wv:\n",
    "            lstOfWordsInWv.append(w)\n",
    "            ctr += 1\n",
    "            wv_res += wv[w]\n",
    "        else:\n",
    "            lstOfWordsNotInWv.append(w)\n",
    "    wv_res = wv_res / ctr\n",
    "    return wv_res\n",
    "\n",
    "def vectorizeContent(inp):\n",
    "    return inp['content'].apply(lambda x: sent_vec(x.split())).to_list()\n",
    "\n",
    "def vectorizeMeta(inp):\n",
    "    lst = []\n",
    "    for i in range(0, inp.shape[0]):\n",
    "        elm = inp.at[i, 'meta_description']\n",
    "        if type(elm) != float: \n",
    "            elm = sent_vec(elm.split())\n",
    "            lst.append(elm)\n",
    "        else:\n",
    "           elm = 'NaN'\n",
    "           lst.append(elm)\n",
    "    return(lst)\n",
    "\n",
    "def binary(inp):\n",
    "    lst = []\n",
    "    for i in range(0, (inp.shape[0])):\n",
    "        colElm = inp.at[i, 'type']\n",
    "        if colElm == 'fake' or colElm == 'conspiracy':\n",
    "            lst.append(1)\n",
    "        else:\n",
    "            lst.append(0)\n",
    "    return lst\n",
    "\n",
    "def connect(input):\n",
    "    lst = []\n",
    "    X_train_meta = vectorizeMeta(input)\n",
    "    X_train_content = vectorizeContent(input)\n",
    "    for i in range (0, len(X_train_content)):\n",
    "        elmMeta = X_train_meta[i]\n",
    "        elmContent = X_train_content[i]\n",
    "        if elmMeta == 'NaN':\n",
    "            lst.append(elmContent)\n",
    "        else:\n",
    "            avg = (elmMeta + elmContent) / 2\n",
    "            lst.append(avg)\n",
    "    return(lst)\n",
    "\n",
    "X_train_meta = connect(train)\n",
    "X_test_meta = connect(test)\n",
    "X_val_meta = connect(val)\n",
    "\n",
    "X_train, y_train = vectorizeContent(train), binary(train)\n",
    "X_test, y_test = vectorizeContent(test), binary(test)\n",
    "X_val, y_val = vectorizeContent(val), binary(val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data overview 3 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatterplot\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m reduced_vectors\n\u001b[0;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mScatterplot\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m scatterPlotOfdata(X_train[:\u001b[39m1000\u001b[39;49m], y_train[:\u001b[39m1000\u001b[39m])\n\u001b[0;32m     37\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWords used for traning:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[39mprint\u001b[39m(lstOfWordsInWv[:\u001b[39m100\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "#from sklearn.decomposition import PCA\n",
    "'''Plots the balance of the dataset'''\n",
    "def balanceCheck():\n",
    "    ones = 0\n",
    "    zeros = 0\n",
    "    for num in y_train:\n",
    "        if num == 1:\n",
    "            ones += 1\n",
    "        elif num == 0:\n",
    "            zeros += 1\n",
    "    #Percentage\n",
    "    total = ones + zeros\n",
    "    percentOnes = round(ones/total * 100)\n",
    "    percentZeros = round(zeros/total * 100)\n",
    "    #Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    labels = ['Fake ' + str(percentOnes) + '%', 'Reliable ' + str(percentZeros) + '%']\n",
    "    size = [ones, zeros]\n",
    "    bar_colors = ['tab:red', 'tab:blue']\n",
    "    ax.bar(labels, size, label=labels, color=bar_colors)\n",
    "\n",
    "'''Creates a scatter plot '''\n",
    "def  scatterPlotOfdata(X, Y):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    reduced_vectors = tsne.fit_transform(X)\n",
    "    scatter_colors = ListedColormap(['red', 'blue'])\n",
    "    plt.scatter(reduced_vectors[:,0], reduced_vectors[:,1], c=Y, cmap=scatter_colors)\n",
    "    plt.show()\n",
    "    return reduced_vectors\n",
    "\n",
    "print(\"Scatterplot\")\n",
    "scatterPlotOfdata(X_train[:1000], y_train[:1000])\n",
    "print(\"Words used for traning:\")\n",
    "print(lstOfWordsInWv[:100])\n",
    "print(\"\\nWords not used for training\")\n",
    "print(lstOfWordsNotInWv[:100])\n",
    "\n",
    "balanceCheck()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE: For dimensionality reduction\n",
    "Scatterplot: There doesn't seem to be any clear patterns on where the position of the points are in correlation to wheter it's fake or reliable. \n",
    "\n",
    "word2vec-GoogleNews-vectors is from 2016, and doesn't contain words such as blockchain and bitcoin. (reference: https://github.com/mmihaltz/word2vec-GoogleNews-vectors)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model #####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
