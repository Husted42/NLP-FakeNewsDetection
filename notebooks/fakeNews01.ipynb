{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tqdm : Progress bar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\alx26\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\alx26\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\alx26\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\alx26\\anaconda3\\lib\\site-packages (from gensim) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "!pip install gensim\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV file, leaving only the collumns we might be interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicates:  8498\n",
      "Fake / reliable\n",
      "Int64Index([   27,    28,    29,    30,    31,    32,    33,    34,    58,\n",
      "               71,\n",
      "            ...\n",
      "            89296, 89297, 89298, 89299, 89300, 89301, 89304, 89307, 94742,\n",
      "            99090],\n",
      "           dtype='int64', length=45768)\n",
      "RangeIndex(start=0, stop=27660, step=1)\n",
      "\n",
      "loaded index\n",
      "RangeIndex(start=0, stop=100000, step=1)\n",
      "\n",
      "filtered\n",
      "       Unnamed: 0      id      type  \\\n",
      "0              27      34      fake   \n",
      "1              28      35      fake   \n",
      "2              29      36      fake   \n",
      "3              30      37      fake   \n",
      "4              31      38      fake   \n",
      "...           ...     ...       ...   \n",
      "73421        7790   96593  reliable   \n",
      "73423        1189  101029  reliable   \n",
      "73424        4169  105092  reliable   \n",
      "73425        4440  105498  reliable   \n",
      "73427        9582  112137  reliable   \n",
      "\n",
      "                                                 content  \\\n",
      "0      Headline: Bitcoin & Blockchain Searches Exceed...   \n",
      "1      Water Cooler 1/25/18 Open Thread; Fake News ? ...   \n",
      "2      Veteran Commentator Calls Out the Growing “Eth...   \n",
      "3      Lost Words, Hidden Words, Otters, Banks and Bo...   \n",
      "4      Red Alert: Bond Yields Are SCREAMING “Inflatio...   \n",
      "...                                                  ...   \n",
      "73421  Amid the debate over whether \"fundamentalist C...   \n",
      "73423  It's the football off-season and Denver Bronco...   \n",
      "73424  It's illegal for Christians to pray in public ...   \n",
      "73425  It's illegal for Christians to pray in public ...   \n",
      "73427  The views expressed by the author do not neces...   \n",
      "\n",
      "                      inserted_at  \\\n",
      "0      2018-02-02 01:19:41.756632   \n",
      "1      2018-02-02 01:19:41.756632   \n",
      "2      2018-02-02 01:19:41.756632   \n",
      "3      2018-02-02 01:19:41.756632   \n",
      "4      2018-02-02 01:19:41.756632   \n",
      "...                           ...   \n",
      "73421  2018-02-02 01:19:41.756632   \n",
      "73423  2018-02-02 01:19:41.756632   \n",
      "73424  2018-02-02 01:19:41.756632   \n",
      "73425  2018-02-02 01:19:41.756632   \n",
      "73427  2018-02-02 01:19:41.756632   \n",
      "\n",
      "                                        meta_description  source  \n",
      "0                                                    NaN     NaN  \n",
      "1                                                    NaN     NaN  \n",
      "2                                                    NaN     NaN  \n",
      "3                                                    NaN     NaN  \n",
      "4                                                    NaN     NaN  \n",
      "...                                                  ...     ...  \n",
      "73421  Amid the debate over whether \"fundamentalist C...     NaN  \n",
      "73423  It's the football off-season and Denver Bronco...     NaN  \n",
      "73424  It's illegal for Christians to pray in public ...     NaN  \n",
      "73425                                                NaN     NaN  \n",
      "73427  This presidential election confronts Christian...     NaN  \n",
      "\n",
      "[64930 rows x 7 columns]\n",
      "\n",
      "Creates a csv file called: readyData.csv\n"
     ]
    }
   ],
   "source": [
    "def getParts():\n",
    "    dropList = ['domain', 'url', 'scraped_at', 'updated_at', 'title', 'authors', 'keywords', 'meta_keywords', 'tags', 'summary' ] # 'source'\n",
    "\n",
    "    df = pd.read_csv('csvFile.csv', nrows=100000)\n",
    "    \n",
    "    # Filter fake\n",
    "    df_fake = df.loc[df['type'] == 'fake']\n",
    "    df_conspiracy = df.loc[df['type'] == 'conspiracy']\n",
    "\n",
    "\n",
    "    #filter reliable\n",
    "    df_reliable = df.loc[df['type'] == 'reliable']\n",
    "    df_political = df.loc[df['type'] == 'political']\n",
    "\n",
    "    #Concat\n",
    "    df_reliable = pd.concat([df_political, df_reliable], ignore_index=True)\n",
    "    df_filtered = pd.concat([df_fake, df_reliable], ignore_index=True)\n",
    "\n",
    "    # Write DataFrame to CSV file\n",
    "    print(\"duplicates: \", df_filtered.duplicated(subset=['content']).sum())\n",
    "    df_filtered = df_filtered.drop(dropList, axis=1)\n",
    "    df_filtered.drop_duplicates(subset=['content'], inplace=True)\n",
    "    df_filtered.to_csv('readyData.csv', index=False)\n",
    "    \n",
    "    print(\"Fake / reliable\")\n",
    "    print(df_fake.index)\n",
    "    print(df_reliable.index)\n",
    "    print(\"\")\n",
    "    print(\"loaded index\")\n",
    "    print(df.index)\n",
    "    print(\"\")\n",
    "    print(\"filtered\")\n",
    "    print(df_filtered)\n",
    "    print(\"\")\n",
    "    print(\"Creates a csv file called: readyData.csv\")\n",
    "getParts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data overview #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types:\n",
      "Unnamed: 0            int64\n",
      "id                    int64\n",
      "type                 object\n",
      "content              object\n",
      "inserted_at          object\n",
      "meta_description     object\n",
      "source              float64\n",
      "dtype: object\n",
      "\n",
      "Count of Null: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0              0\n",
       "id                      0\n",
       "type                    0\n",
       "content                 0\n",
       "inserted_at             0\n",
       "meta_description    50357\n",
       "source              64930\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data01 = pd.read_csv('readyData.csv')\n",
    "def types(inp):\n",
    "    results = inp\n",
    "    results = results.dtypes\n",
    "    return results\n",
    "\n",
    "print(\"Types:\")\n",
    "print(types(data01))\n",
    "\n",
    "print(\"\")\n",
    "print ('Count of Null: ' )\n",
    "data01.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preproccesing #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### -- Imports -- #####\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Remember to run the line below the first time \n",
    "# nltk.download('punkt') \n",
    "\n",
    "fileRaw = 'readyData.csv'\n",
    "\n",
    "\n",
    "''' Creates dataframe (Run fuctions) '''\n",
    "def createDataframe(input): \n",
    "    df = pd.read_csv(input)\n",
    "    df = cleanContent(df, 'content')\n",
    "    return df\n",
    "\n",
    "''' Cleans and tokenizes text  '''\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "def cleanContent(input, columnName):\n",
    "    input[columnName] = input[columnName].str.lower()\n",
    "    regexList = ['\\.', ':', '&', ',', '\\?', ' us ', '!', ';', '\\$', '%', '\\(', '\\)', '\\[', '\\]']\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    regexList += [r'\\b{}\\b'.format(word) for word in stop_words]\n",
    "    pattern = re.compile('|'.join(regexList))\n",
    "    input[columnName] = input[columnName].apply(lambda x: pattern.sub('', x))\n",
    "    input[columnName] = input[columnName].str.replace(r'\\b\\d+-\\d+-\\d+-\\d+\\b', 'phone', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r'https?://\\S+|\\bhttp://\\S+', 'url', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r'\\b\\d+\\b', 'number', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r\"\\b\\w\\b\\s?\\b\", '', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r\"['`.*@-]\", '', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r'\\s+', ' ', regex=True)\n",
    "    for i in range(0, len(input[columnName])):\n",
    "        # print (input.at[i, columnName]) \n",
    "        colElm = input.at[i, columnName]\n",
    "        colElm = nltk.word_tokenize(colElm)\n",
    "        stemmed_words = []\n",
    "        for word in colElm:\n",
    "            stemmed_words.append(sno.stem(word))\n",
    "        stemmed_words = ' '.join(stemmed_words)\n",
    "        input.at[i, columnName] = stemmed_words\n",
    "    return input\n",
    "\n",
    "'''Converts to csv File'''\n",
    "def run(inp):\n",
    "    inp = inp.to_csv('cleanedNews.csv', index = True)\n",
    "\n",
    "##### -- Calls -- #####\n",
    "run(createDataframe(fileRaw))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content overview #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words in clean and raw\n",
      "['number', '’', '“', '”', 'one', 'trump', 'state', 'would', 'peopl', 'time', 'blockchain', 'year', 'like', 'new', 'said', 'use', 'also', '–', 'report', 'make', 'next', 'get', 'market', 'two', 'go', '01:19:41.756632,,', 'even', 'think', 'stock', 'say', 'first', 'work', '``', 'day', 'presid', 'mani', 'govern', 'search', 'world', 'bitcoin', 'american', 'right', 'take', 'could', 'need', 'come', '—', 'way', 'exceed', 'know', 'nation', 'sourc', 'see', 'may', 'headlin', 'call', 'fact', \"''\", 'want', 'the', 'includ', 'stori', 'news', 'last', 'thing', 'well', 'countri', 'much', 'support', 'look', 'back', 'obama', 'hous', 'law', 'live', 'democrat', 'good', 'help', 'polit', 'public', '‘', 'show', 'power', 'week', 'republican', 'unit', 'made', 'war', 'chang', 'life', 'part', 'group', 'follow', 'inform', 'read', 'end', 'post', 'system', 'america', 'point']\n",
      "['the', 'of', 'to', 'and', 'a', 'in', 'is', 'that', 'for', 'on', 'are', 'with', 'as', 'it', 'this', 'be', 'by', 'was', 'i', 'have', 'you', 'from', 'at', 'not', 'or', 'they', 'we', 'he', 'has', 'but', 'an', 'will', 'his', 'their', 'who', 'all', 'your', 'more', 'about', 'if', 'can', 'one', 'our', 'what', 'would', 'blockchain', 'which', 'been', 'when', 'were', 'so', 'out', 'new', 'there', 'people', 'had', 'my', '–', 'up', 'no', 'also', 'than', 'its', 'some', 'just', 'like', 'other', 'into', 'do', '&', 'these', 'two', '01:19:41.756632,,', 'over', 'how', 'after', 'only', 'us', 'even', 'any', 'most', 'said', 'because', 'many', 'those', 'bitcoin', 'stocks', 'first', 'time', 'could', 'trump', 'get', 'her', 'them', 'think', 'now', 'exceed', 'searches', 'it’s', 'trump!']\n",
      "\n",
      "Words that apear in both list of words:\n",
      "['one', 'trump', 'would', 'time', 'blockchain', 'like', 'new', 'said', 'also', '–', 'get', 'two', '01:19:41.756632,,', 'even', 'think', 'first', 'bitcoin', 'could', 'exceed', 'the']\n"
     ]
    }
   ],
   "source": [
    "##### -- Imports -- #####\n",
    "import matplotlib.pyplot as plt \n",
    "import itertools\n",
    "\n",
    "##### --  Global variables -- #####\n",
    "fileCleaned = 'cleanedNews.csv'\n",
    "\n",
    "\n",
    "\n",
    "# dfClean = pd.read_csv(nameOfCleanedCSV)\n",
    "# dfRaw = pd.read_csv(nameOfRawCSV)\n",
    "\n",
    "##### --  Functions -- #####\n",
    "''' Creates dictionary of 100 most used times'''\n",
    "def wordDic(input):\n",
    "    file = open(input, 'r',  errors=\"surrogateescape\")\n",
    "    read = file.read().lower()\n",
    "    words = read.split()  \n",
    "    dictionary = {}\n",
    "    for i in words:\n",
    "        if i in dictionary:\n",
    "            dictionary[i] += 1  \n",
    "        else:\n",
    "            dictionary[i] = 1\n",
    "    sort = dict(sorted(dictionary.items(), key=lambda x: x[1], reverse=True))\n",
    "    words = dict(itertools.islice(sort.items(), 100))\n",
    "    return words\n",
    "\n",
    "def something(input):\n",
    "    resultList = list(wordDic(input).items())\n",
    "    lst = []\n",
    "    for elm in resultList:\n",
    "        lst.append(elm[0])\n",
    "    return lst\n",
    "\n",
    "\n",
    "##### --  Prints -- #####\n",
    "print(\"Most used words in clean and raw\")\n",
    "mostUsedWordsClean = (something(fileCleaned))\n",
    "mostUsedWordsRaw = (something(fileRaw))\n",
    "print(mostUsedWordsClean)\n",
    "print(mostUsedWordsRaw) \n",
    "\n",
    "print(\"\\nWords that apear in both list of words:\")\n",
    "doubleAppearances = []\n",
    "for word in mostUsedWordsClean:\n",
    "    if word in mostUsedWordsRaw:\n",
    "        doubleAppearances.append(word)\n",
    "print(doubleAppearances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code block solves the following tasks:\n",
    " - Cheks the effect of the cleaning, based on the words apearing in the most used list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word number apears: 587958 times\n",
      "the word phone apears: 6999 times\n",
      "the word url apears: 160 times\n"
     ]
    }
   ],
   "source": [
    "def allWordsToDic(input):\n",
    "    file = open(input, 'r',  errors=\"surrogateescape\")\n",
    "    read = file.read().lower()\n",
    "    words = read.split()  \n",
    "    dictionary = {}\n",
    "    for i in words:\n",
    "        if i in dictionary:\n",
    "            dictionary[i] += 1  \n",
    "        else:\n",
    "            dictionary[i] = 1\n",
    "    return dictionary\n",
    "\n",
    "print(\"the word number apears: \" + str(allWordsToDic(fileCleaned)['number']) + ' times')\n",
    "print(\"the word phone apears: \" + str(allWordsToDic(fileCleaned)['phone']) + ' times')\n",
    "print(\"the word url apears: \" + str(allWordsToDic(fileCleaned)['url']) + ' times')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ready data for baseline model #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "80% of DataFrame:\n",
      "(51944, 8)\n",
      "\n",
      "10% of DataFrame:\n",
      "(6493, 8)\n",
      "\n",
      "rest of the 10% of DataFrame:\n",
      "(6493, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "df = pd.read_csv(('cleanedNews.csv'))\n",
    " \n",
    "# Creating a dataframe with 80% of the data\n",
    "part_80 = df.sample(frac = 0.8)\n",
    "part_80.to_csv('split80_Train.csv', index=False)\n",
    "\n",
    "# Creating a dataframe with the rest (20%)\n",
    "rest_part = df.drop(part_80.index) \n",
    "rest_part.to_csv('split20_temp.csv', index=False)\n",
    "\n",
    "# Creating a new dataframe to split the 20 % for test and validation\n",
    "df10 = pd.read_csv('split20_temp.csv')\n",
    "\n",
    "# Creating a dataframe with 50% of the data / 10% of the whole dataset \n",
    "part_50 = df10.sample(frac=0.5)\n",
    "part_50.to_csv('split10_test.csv', index=False)\n",
    "\n",
    "# Creating a dataframe with 50% of the data / 10% of the whole dataset \n",
    "part_50_2 = df10.drop(part_50.index)\n",
    "part_50_2.to_csv('split10_val.csv', index=False)\n",
    "\n",
    "print(\"\\n80% of DataFrame:\")\n",
    "print(part_80.shape)\n",
    "\n",
    "print(\"\\n10% of DataFrame:\")\n",
    "print(part_50.shape)\n",
    "\n",
    "print(\"\\nrest of the 10% of DataFrame:\")\n",
    "print(part_50_2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
