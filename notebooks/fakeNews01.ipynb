{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports used. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "#pip install gensim\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt \n",
    "import itertools\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "import gensim.downloader as api\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loads CSV file, leaving only the columns we might be interested in. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' reads data from a CSV file, filters it by type, removes duplicates and unnecessary columns, and writes the filtered data to a new CSV file.'''\n",
    "##### -- Functions -- #####\n",
    "def getParts():\n",
    "    dropList = ['domain', 'url', 'scraped_at', 'updated_at', 'title', 'authors', 'keywords', 'meta_keywords', 'tags', 'summary' ] # 'source'\n",
    "\n",
    "    df = pd.read_csv('csvFile.csv', nrows=100000)\n",
    "    \n",
    "    # Filter fake\n",
    "    df_fake = df.loc[df['type'] == 'fake']\n",
    "    df_conspiracy = df.loc[df['type'] == 'conspiracy']\n",
    "\n",
    "\n",
    "    #filter reliable\n",
    "    df_reliable = df.loc[df['type'] == 'reliable']\n",
    "    df_political = df.loc[df['type'] == 'political']\n",
    "\n",
    "    #Concat\n",
    "    df_reliable = pd.concat([df_political, df_reliable], ignore_index=True)\n",
    "    df_fake = pd.concat([df_conspiracy, df_fake], ignore_index=True)\n",
    "    df_filtered = pd.concat([df_fake, df_reliable], ignore_index=True)\n",
    "\n",
    "    # Write DataFrame to CSV file\n",
    "    print(\"duplicates: \", df_filtered.duplicated(subset=['content']).sum())\n",
    "    df_filtered = df_filtered.drop(dropList, axis=1)\n",
    "    df_filtered.drop_duplicates(subset=['content'], inplace=True)\n",
    "    df_filtered.to_csv('readyData.csv', index=False)\n",
    "    \n",
    "    ##### -- Print -- #####\n",
    "    print(\"Fake / reliable\")\n",
    "    print(df_fake.index)\n",
    "    print(df_reliable.index)\n",
    "    print(\"\")\n",
    "    print(\"loaded index\")\n",
    "    print(df.index)\n",
    "    print(\"\")\n",
    "    print(\"filtered\")\n",
    "    print(df_filtered)\n",
    "    print(\"\")\n",
    "    print(\"Creates a csv file called: readyData.csv\")\n",
    "getParts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data overview 1 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types:\n",
      "Unnamed: 0            int64\n",
      "id                    int64\n",
      "type                 object\n",
      "content              object\n",
      "inserted_at          object\n",
      "meta_description     object\n",
      "source              float64\n",
      "dtype: object\n",
      "\n",
      "Count of Null: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0              0\n",
       "id                      0\n",
       "type                    0\n",
       "content                 0\n",
       "inserted_at             0\n",
       "meta_description    54822\n",
       "source              70094\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data01 = pd.read_csv('readyData.csv')\n",
    "\n",
    "##### -- Functions -- #####\n",
    "''' Prints the types data in each column'''\n",
    "def types(inp):\n",
    "    results = inp\n",
    "    results = results.dtypes\n",
    "    return results\n",
    "\n",
    "\n",
    "#####  -- prints -- #####\n",
    "print(\"Types:\")\n",
    "print(types(data01))\n",
    "print(\"\")\n",
    "print ('Count of Null: ' )\n",
    "data01.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocceses the data, preparing it for data exploration and data proccesing. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Remember to run the line below the first time \n",
    "# nltk.download('punkt') \n",
    "\n",
    "### Variables ###\n",
    "fileRaw = 'readyData.csv'\n",
    "\n",
    "\n",
    "### Functions ###\n",
    "''' Creates dataframe (Run fuctions) '''\n",
    "def createDataframe(input): \n",
    "    df = pd.read_csv(input)\n",
    "    df = cleanContent(df, 'content')\n",
    "    df = cleanContent(df, 'meta_description')\n",
    "    return df\n",
    "\n",
    "''' Cleans and tokenizes text  '''\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "def cleanContent(input, columnName):\n",
    "    input[columnName] = input[columnName].str.lower()\n",
    "    regexList = ['\\.', ':', '&', ',', '\\?', ' us ', '!', ';', '\\$', '%', '\\(', '\\)', '\\[', '\\]']\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    regexList += [r'\\b{}\\b'.format(word) for word in stop_words]\n",
    "    pattern = re.compile('|'.join(regexList))\n",
    "    input[columnName] = input[columnName].apply(lambda x: pattern.sub('', x))\n",
    "    input[columnName] = input[columnName].str.replace(r'\\b\\d+-\\d+-\\d+-\\d+\\b', 'phone', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r'https?://\\S+|\\bhttp://\\S+', 'url', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r'\\b\\d+\\b', 'number', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r\"\\b\\w\\b\\s?\\b\", '', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r\"['`.*@-]\", '', regex=True)\n",
    "    input[columnName] = input[columnName].str.replace(r'\\s+', ' ', regex=True)\n",
    "    for i in range(0, len(input[columnName])):\n",
    "        colElm = input.at[i, columnName]\n",
    "        if not colElm.isspace() and colElm != \"\":\n",
    "            colElm = nltk.word_tokenize(colElm)\n",
    "            stemmed_words = []\n",
    "            for word in colElm:\n",
    "                stemmed_words.append(sno.stem(word))\n",
    "            stemmed_words = ' '.join(stemmed_words)\n",
    "            input.at[i, columnName] = stemmed_words\n",
    "    return input\n",
    "\n",
    "'''Converts to csv File'''\n",
    "def run(inp):\n",
    "    inp = inp.to_csv('cleanedNews.csv', index = True)\n",
    "\n",
    "##### -- Calls -- #####\n",
    "run(createDataframe(fileRaw))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data overview 2 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words in clean and raw\n",
      "['number', '’', '“', '”', 'one', 'trump', 'state', 'peopl', 'would', 'time', 'year', 'like', 'blockchain', 'new', 'said', 'use', 'also', '–', 'report', 'make', 'get', 'go', 'next', 'even', 'two', '``', 'market', '01:19:41.756632,nan,', 'presid', 'govern', 'say', 'think', 'american', 'mani', '—', 'work', 'first', 'day', 'world', 'right', 'stock', 'nation', 'could', 'take', 'come', 'way', \"''\", 'need', 'may', 'know', 'search', 'see', 'bitcoin', 'call', 'want', 'sourc', 'exceed', 'fact', 'obama', 'includ', 'headlin', 'thing', 'countri', 'well', 'last', 'stori', 'news', 'much', 'support', 'back', 'hous', 'law', 'live', 'look', 'polit', 'democrat', 'good', 'unit', '‘', 'power', 'public', 'help', 'show', 'war', 'week', 'america', 'republican', 'follow', 'chang', 'made', 'life', 'group', 'part', 'end', 'person', 'inform', 'system', 'sinc', 'point', 'continu']\n",
      "['the', 'of', 'to', 'and', 'a', 'in', 'is', 'that', 'for', 'on', 'are', 'as', 'with', 'it', 'this', 'be', 'by', 'was', 'have', 'i', 'you', 'from', 'at', 'not', 'they', 'or', 'we', 'has', 'he', 'but', 'an', 'will', 'his', 'their', 'who', 'all', 'more', 'about', 'your', 'if', 'one', 'can', 'our', 'what', 'would', 'been', 'which', 'when', 'were', 'so', 'blockchain', 'out', 'there', 'people', 'new', 'had', 'no', 'my', 'up', 'than', 'its', '–', 'also', 'just', 'other', 'some', 'like', 'do', 'into', 'these', '&', 'over', 'two', '01:19:41.756632,,', 'how', 'us', 'only', 'any', 'after', 'even', 'most', 'many', 'because', 'said', 'those', '—', 'time', 'could', 'now', 'them', 'first', 'get', 'trump', 'her', 'may', 'it’s', 'being', 'think', 'bitcoin', 'stocks']\n",
      "\n",
      "Words that apear in both list of words:\n",
      "['one', 'trump', 'would', 'time', 'like', 'blockchain', 'new', 'said', 'also', '–', 'get', 'even', 'two', 'think', '—', 'first', 'could', 'may', 'bitcoin']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##### --  Global variables -- #####\n",
    "fileCleaned = 'cleanedNews.csv'\n",
    "\n",
    "\n",
    "\n",
    "# dfClean = pd.read_csv(nameOfCleanedCSV)\n",
    "# dfRaw = pd.read_csv(nameOfRawCSV)\n",
    "\n",
    "##### --  Functions -- #####\n",
    "''' Creates dictionary of 100 most used times'''\n",
    "def wordDic(input):\n",
    "    file = open(input, 'r',  errors=\"surrogateescape\")\n",
    "    read = file.read().lower()\n",
    "    words = read.split()  \n",
    "    dictionary = {}\n",
    "    for i in words:\n",
    "        if i in dictionary:\n",
    "            dictionary[i] += 1  \n",
    "        else:\n",
    "            dictionary[i] = 1\n",
    "    sort = dict(sorted(dictionary.items(), key=lambda x: x[1], reverse=True))\n",
    "    words = dict(itertools.islice(sort.items(), 100))\n",
    "    return words\n",
    "\n",
    "\n",
    "''' Creates list of 100 most used words'''\n",
    "def something(input):\n",
    "    resultList = list(wordDic(input).items())\n",
    "    lst = []\n",
    "    for elm in resultList:\n",
    "        lst.append(elm[0])\n",
    "    return lst\n",
    "\n",
    "\n",
    "##### --  Prints -- #####\n",
    "print(\"Most used words in clean and raw\")\n",
    "mostUsedWordsClean = (something(fileCleaned))\n",
    "mostUsedWordsRaw = (something(fileRaw))\n",
    "print(mostUsedWordsClean)\n",
    "print(mostUsedWordsRaw) \n",
    "print(\"\\nWords that apear in both list of words:\")\n",
    "doubleAppearances = []\n",
    "for word in mostUsedWordsClean:\n",
    "    if word in mostUsedWordsRaw:\n",
    "        doubleAppearances.append(word)\n",
    "print(doubleAppearances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code block solves the following tasks:\n",
    " - Checks the effect of the cleaning, based on the words apearing in the most used list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converts all words to lowercase and creates a dictionary with the count of each word, and then prints the number of times specific words appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the word number apears: 668333 times\n",
      "the word phone apears: 7696 times\n",
      "the word url apears: 168 times\n"
     ]
    }
   ],
   "source": [
    "##### --  Functions -- #####\n",
    "''' Takes all the data and converts it to a dictionary'''\n",
    "def allWordsToDic(input):\n",
    "    file = open(input, 'r',  errors=\"surrogateescape\")\n",
    "    read = file.read().lower()\n",
    "    words = read.split()  \n",
    "    dictionary = {}\n",
    "    for i in words:\n",
    "        if i in dictionary:\n",
    "            dictionary[i] += 1  \n",
    "        else:\n",
    "            dictionary[i] = 1\n",
    "    return dictionary\n",
    "\n",
    "##### --  Prints -- #####\n",
    "print(\"the word number apears: \" + str(allWordsToDic(fileCleaned)['number']) + ' times')\n",
    "print(\"the word phone apears: \" + str(allWordsToDic(fileCleaned)['phone']) + ' times')\n",
    "print(\"the word url apears: \" + str(allWordsToDic(fileCleaned)['url']) + ' times')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splits and readies data for baseline model. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "80% of DataFrame:\n",
      "(56075, 8)\n",
      "\n",
      "10% of DataFrame:\n",
      "(7010, 8)\n",
      "\n",
      "rest of the 10% of DataFrame:\n",
      "(7009, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "df = pd.read_csv(('cleanedNews.csv'))\n",
    " \n",
    "# Creating a dataframe with 80% of the data\n",
    "part_80 = df.sample(frac = 0.8)\n",
    "part_80.to_csv('split80_train.csv', index=False)\n",
    "\n",
    "# Creating a dataframe with the rest (20%)\n",
    "rest_part = df.drop(part_80.index) \n",
    "rest_part.to_csv('split20_temp.csv', index=False)\n",
    "\n",
    "# Creating a new dataframe to split the 20 % for test and validation\n",
    "df10 = pd.read_csv('split20_temp.csv')\n",
    "\n",
    "# Creating a dataframe with 50% of the data / 10% of the whole dataset \n",
    "part_50 = df10.sample(frac=0.5)\n",
    "part_50.to_csv('split10_test.csv', index=False)\n",
    "\n",
    "# Creating a dataframe with 50% of the data / 10% of the whole dataset \n",
    "part_50_2 = df10.drop(part_50.index)\n",
    "part_50_2.to_csv('split10_val.csv', index=False)\n",
    "\n",
    "\n",
    "##### --  Prints -- #####\n",
    "\n",
    "print(\"\\n80% of DataFrame:\")\n",
    "print(part_80.shape)\n",
    "\n",
    "print(\"\\n10% of DataFrame:\")\n",
    "print(part_50.shape)\n",
    "\n",
    "print(\"\\nrest of the 10% of DataFrame:\")\n",
    "print(part_50_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepares the split data, such that a model can be trained. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### -- variables -- #####\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "train = pd.read_csv('split80_train.csv')\n",
    "test = pd.read_csv('split10_test.csv')\n",
    "val = pd.read_csv('split10_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defines several functions to vectorize text data and create binary labels based on the 'type' column, and then applies these functions to train, test, and validation datasets to create vectorized feature sets and binary labels for each dataset #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstOfWordsInWv = []\n",
    "lstOfWordsNotInWv = []\n",
    "\n",
    "##### -- Functions -- #####\n",
    "\n",
    "def sent_vec(sent):\n",
    "    vector_size = wv.vector_size\n",
    "    wv_res = np.zeros(vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in wv:\n",
    "            lstOfWordsInWv.append(w)\n",
    "            ctr += 1\n",
    "            wv_res += wv[w]\n",
    "        else:   \n",
    "            lstOfWordsNotInWv.append(w)\n",
    "    wv_res = wv_res / ctr\n",
    "    return wv_res\n",
    "\n",
    "''' applies sent_vec to content and returns a list '''\n",
    "def vectorizeContent(inp):\n",
    "    return inp['content'].apply(lambda x: sent_vec(x.split())).to_list()\n",
    "\n",
    "\n",
    "''' applies sent_vec to meta description and returns a list '''\n",
    "def vectorizeMeta(inp):\n",
    "    lst = []\n",
    "    for i in range(0, inp.shape[0]):\n",
    "        elm = inp.at[i, 'meta_description']\n",
    "        if type(elm) != float: \n",
    "            elm = sent_vec(elm.split())\n",
    "            lst.append(elm)\n",
    "        else:\n",
    "           elm = 'NaN'\n",
    "           lst.append(elm)\n",
    "    return(lst)\n",
    "\n",
    "''' creates a binary list based on the 'type' column '''\n",
    "def binary(inp):\n",
    "    lst = []\n",
    "    for i in range(0, (inp.shape[0])):\n",
    "        colElm = inp.at[i, 'type']\n",
    "        if colElm == 'fake' or colElm == 'conspiracy':\n",
    "            lst.append(1)\n",
    "        else:\n",
    "            lst.append(0)\n",
    "    return lst\n",
    "\n",
    "\n",
    "''' connects vectorizeMeta and vectorizeContent to return a list '''\n",
    "def connect(input):\n",
    "    lst = []\n",
    "    X_train_meta = vectorizeMeta(input)\n",
    "    X_train_content = vectorizeContent(input)\n",
    "    for i in range (0, len(X_train_content)):\n",
    "        elmMeta = X_train_meta[i]\n",
    "        elmContent = X_train_content[i]\n",
    "        if elmMeta == 'NaN':\n",
    "            lst.append(elmContent)\n",
    "        else:\n",
    "            avg = (elmMeta + elmContent) / 2\n",
    "            lst.append(avg)\n",
    "    return(lst)\n",
    "\n",
    "##### -- variables -- #####\n",
    "X_train_meta = connect(train)\n",
    "X_test_meta = connect(test)\n",
    "X_val_meta = connect(val)\n",
    "\n",
    "X_train, y_train = vectorizeContent(train), binary(train)\n",
    "X_test, y_test = vectorizeContent(test), binary(test)\n",
    "X_val, y_val = vectorizeContent(val), binary(val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data overview 3 - Data visualization #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### -- functions -- #####\n",
    "'''Plots the balance of the dataset'''\n",
    "def balanceCheck():\n",
    "    ones = 0\n",
    "    zeros = 0\n",
    "    for num in y_train:\n",
    "        if num == 1:\n",
    "            ones += 1\n",
    "        elif num == 0:\n",
    "            zeros += 1\n",
    "    #Percentage\n",
    "    total = ones + zeros\n",
    "    percentOnes = round(ones/total * 100)\n",
    "    percentZeros = round(zeros/total * 100)\n",
    "    #Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    labels = ['Fake ' + str(percentOnes) + '%', 'Reliable ' + str(percentZeros) + '%']\n",
    "    size = [ones, zeros]\n",
    "    bar_colors = ['tab:red', 'tab:blue']\n",
    "    ax.bar(labels, size, label=labels, color=bar_colors)\n",
    "\n",
    "'''Creates a scatter plot '''\n",
    "def  scatterPlotOfdata(X, Y):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    reduced_vectors = tsne.fit_transform(X)\n",
    "    scatter_colors = ListedColormap(['red', 'blue'])\n",
    "    plt.scatter(reduced_vectors[:,0], reduced_vectors[:,1], c=Y, cmap=scatter_colors)\n",
    "    plt.show()\n",
    "    return reduced_vectors\n",
    "\n",
    "##### --  Prints -- #####\n",
    "print(\"Scatterplot\")\n",
    "scatterPlotOfdata(X_train[:1000], y_train[:1000])\n",
    "print(\"Words used for traning:\")\n",
    "print(lstOfWordsInWv[:100])\n",
    "print(\"\\nWords not used for training\")\n",
    "print(lstOfWordsNotInWv[:100])\n",
    "\n",
    "balanceCheck()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE: For dimensionality reduction\n",
    "Scatterplot: There doesn't seem to be any clear patterns on where the position of the points are in correlation to wheter it's fake or reliable. \n",
    "\n",
    "word2vec-GoogleNews-vectors is from 2016, and doesn't contain words such as blockchain and bitcoin. (reference: https://github.com/mmihaltz/word2vec-GoogleNews-vectors)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model #####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "864f8d8b4840020f26ec67225d5591c64f76831423ee6ca9a625bc7dce437ae7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
